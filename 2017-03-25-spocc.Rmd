---
layout: post
title: "Mapping waxwings annual migration without Twitter"
comments: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE, 
                      cache = FALSE)
```

Recently a reader left a comment on this blog mentioning [his cool blog post](https://robbriers.github.io/post/2017-03-14-mapping-the-spread-of-a-migratory-bird-using-twitter/) in which he mapped the spread of a migratory bird using Twitter. His data source was [the Waxwings UK account](https://twitter.com/WaxwingsUK) which reports sightings of Waxwings in the UK. I decided to try reproducing and extending his work using [the rOpenSci `spocc` package](https://github.com/ropensci/spocc) that interfaces different sources of species occurrence data.

<!--more-->

# Getting the occurrence data

As mentioned above I got the data via `spocc`. I read the README of the Github repo and learnt that it's called `spocc` like *sp*ecies *occ*urrence data. So now I should never forget how many "c" there are in the word _occurrence_. Now please send help for my remembering it has two "r".

The `spocc` package interacts with so many data sources that I felt a bit overwhelmed. I guess ecologists are not often blessed with so much data. Note that I don't limit my query to the UK. Indeed, since seeing [this map](https://en.wikipedia.org/wiki/Bohemian_waxwing#/media/File:Bombycillagarrulusmap2.png) my interest for the bohemian waxwing became global. 

I decided to use only data from GBIF, and to get data for different years and all 3 species of waxwings, which meant a lot of data so for getting it I sliced the Earth (I like how evil this sentence makes me sound, ah!). Note that I used overlapping slices because otherwise I didn't get data at the limits between slices. I guess I could have made my slices slightly less overlapping though.

```r
library("spocc")
library("dplyr")
library("purrr")
library("scrubr")
get_slice <- function(longitude, year){
  print(paste(year, longitude))
  gbifopts <- list(limit = 200000,
                   year = year)
  waxwings <- occ(query = "Bombycilla",
                  from = c('gbif'), 
                  gbifopts = gbifopts,
                  geometry = c(longitude - 0.5, - 90, 
                               longitude + 0.5, 90))
  waxwings <- occ2df(waxwings)
}
longitudes <- seq( -180, 179, by = 0.5)
years <- rep(2011:2016, length(lo ngitudes))
longitudes <- rep(longitudes,  6)
waxwings <- map2(longitudes, years, get_slice)
for (i in 1:length(waxwings)){
  waxwings[[i]]$latitude <- as.numeric(waxwings[[i]]$latitude)
  waxwings[[i]]$longitude <- as.numeric(waxwings[[i]]$longitude)
}

waxwings <- bind_rows(waxwings)
waxwings <- unique(waxwings)

waxwings <- filter(waxwings, !is.na(name))
readr::write_csv(waxwings, path = "uncleaned_waxwings.csv")


```

# Cleaning the occurrence data

Now because my MSc of ecology is far behind me (but still close to my heart!) I have no idea how to assess the quality of species occurrence data. And even if I did I would have been delighted by the discovery of another rOpenSci package, [`scrubr`](https://github.com/ropensci/scrubr), whose aim is to clean species occurrence records. Because I had so much data, I cleaned each day separately, otherwise it was just too long and hard on my poor computer. Don't start thinking `scrubr` is slow, it isn't and it's getting faster by the minute [thanks to work by its maintainer](https://github.com/ropensci/scrubr/issues/26).

```r
cleanup <- function(df){
  print(df$date[1])
  df <- df %>%
    coord_impossible() %>%
    coord_incomplete() %>%
    coord_unlikely() 

    if(nrow(df) > 1){
    
      df <- dedup(df)
    }
  df <- df %>%
    date_standardize("%Y-%m-%d") %>%
    date_missing()
    return(df)
} 
waxwings <- readr::read_csv("uncleaned_waxwings.csv")


waxwings <- split(waxwings, 
                  waxwings$date)

waxwings <- lapply(waxwings, cleanup)

waxwings <- bind_rows(waxwings)
waxwings <- unique(waxwings)
readr::write_csv(waxwings, path = "waxwings.csv")
```

```{r, eval = TRUE, echo = FALSE}
waxwings <- readr::read_csv("waxwings.csv")


```

I removed the records with impossible, incomplete or unlikely coordinates (unlikely being e.g. a political centroid, impossible coordinates with a too high longitude), I also removed duplicate records and records without a data. This was so easy! I'd like the `scrubr` package to come clean my flat, as a team with the [`janitor` package](https://github.com/sfirke/janitor). Now, in real life, I'd probably be even stricter with data cleaning but for the scope of this blog post, using that package without any additionnal check was enough. After all this glorious data cleaning, I had `r nrow(waxwings)` records. 

# Exploring the data

## Number of occurrences by species over time

Seasonality (fewer people where they breed)

## Day-of-the-week effects

More occurrence reports in the week-ends. Mention the bizday package I could have used for finer characterization in particular in the US (because taking into account different holidays in different countries, huh.)

# Mapping the migrations!

Animated maps

replace ugly red points by bird emojis

maybe try spatial smoothing by month... or mention it as a possible extension.

maybe use a better projection.

# Concluding

I was impressed by rOpenSci tools for getting and cleaning occurrence data. 